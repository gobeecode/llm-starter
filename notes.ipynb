{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ebb039c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4fe05eb",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9d41a",
   "metadata": {},
   "source": [
    "- GPT4o - Technically better model from openai.\n",
    "- Claude - More ethical model from anthropic.\n",
    "- Gemini - \n",
    "- Llama - Opensource model from meta.\n",
    "- Gemma - Opensource model from Google.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9325c",
   "metadata": {},
   "source": [
    "### Transition of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ccce8",
   "metadata": {},
   "source": [
    "- Initially the models were interpreted character by character which is very power consuming and tedious. \n",
    "- Then the models were trained to interpret word by word which is better than the previous approach but still needs to store a lot of vocabulary including all the related words.\n",
    "- The concept of tokens took the best of both worlds and solves the problems from each of these approaches. Tokens break the words into subwords and interpret the meaning from the tokenized words."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
